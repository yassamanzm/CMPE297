{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import enchant\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "df_train = pd.read_csv(\"/home/yassaman/git/CMPE297/FinalProject/test_mini.csv\").fillna(\"empty\")\n",
    "df_train.question1 = df_train.question1.astype(str)\n",
    "df_train.question2 = df_train.question2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from string import digits\n",
    "words = []\n",
    "document1 = []\n",
    "document2 = []\n",
    "for index, row in df_train.iterrows():\n",
    "    row['question1'].translate(None, string.punctuation)\n",
    "    row['question2'].translate(None, string.punctuation)\n",
    "    row['question1'] = re.sub(r\"[^A-Za-z0-9]\", \" \", row['question1'])\n",
    "    row['question2'] = re.sub(r\"[^A-Za-z0-9]\", \" \", row['question2'])\n",
    "    row['question1'] = re.sub('[!\\[\\]@#\\\\\\.?,$/]', '', row['question1'])\n",
    "    row['question2'] = re.sub('[!\\[\\]@#\\\\\\.?,$/]', '', row['question2'])\n",
    "#     remove_digits = str.maketrans('', '', digits)\n",
    "    row['question1'] = row['question1'].translate(None, digits)\n",
    "    row['question2'] = row['question2'].translate(None, digits)\n",
    "    word_list = nltk.word_tokenize(row['question1'])\n",
    "    word_list2 = nltk.word_tokenize(row['question2'])\n",
    "    # add to our words list\n",
    "    for w in word_list:\n",
    "        w = SnowballStemmer(\"english\").stem(w)\n",
    "        words.append(w)\n",
    "        # add to documents in our corpus\n",
    "        document1.append(w)\n",
    "    for w2 in word_list2:\n",
    "        w2 = SnowballStemmer('english').stem(w2)\n",
    "        words.append(w2)\n",
    "        # add to documents in our corpus\n",
    "        document2.append(w2)\n",
    "words = list(set(words))\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "for w in words:\n",
    "    if(d.check(w)==False):\n",
    "        if(d.suggest(w) is not None):\n",
    "            try:\n",
    "                w = d.suggest(w)[0]\n",
    "            except IndexError:\n",
    "                w=w\n",
    "                print 'Error ' + w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'what', 'is', u'the', u'step', 'by', u'step', u'guid', 'to', u'invest', 'in', u'share', u'market', 'in', u'india', u'what', 'is', u'the', u'stori', 'of', u'kohinoor', u'koh', 'i', u'noor', u'diamond', u'how', u'can', 'i', u'increas', u'the', u'speed', 'of', 'my', u'internet', u'connect', u'while', u'use', 'a', u'vpn', u'whi', 'am', 'i', u'mental', u'veri', u'lone', u'how', u'can', 'i', u'solv', 'it', u'which', u'one', u'dissolv', 'in', u'water', u'quik', u'sugar', u'salt', u'methan', u'and', u'carbon', 'di', u'oxid', u'astrolog', 'i', 'am', 'a', u'capricorn', u'sun', u'cap', u'moon', u'and', u'cap', u'rise', u'what', u'doe', u'that', u'say', u'about', 'me', u'should', 'i', u'buy', u'tiago', u'how', u'can', 'i', 'be', 'a', u'good', u'geologist', u'when', 'do', u'you', u'use', u'instead', 'of', u'motorola', u'compani', u'can', 'i', u'hack', 'my', u'charter', u'motorolla', u'dcx', u'method', 'to', u'find', u'separ', 'of', u'slit', u'use', u'fresnel', u'biprism', u'how', 'do', 'i', u'read', u'and', u'find', 'my', u'youtub', u'comment', u'what', u'can', u'make', u'physic', u'easi', 'to', u'learn', u'what', u'was', u'your', u'first', u'sexual', u'experi', u'like', u'what', u'are', u'the', u'law', 'to', u'chang', u'your', u'status', u'from', 'a', u'student', u'visa', 'to', 'a', u'green', u'card', 'in', u'the', 'us', u'how', 'do', u'they', u'compar', 'to', u'the', u'immigr', u'law', 'in', u'canada', u'what', u'would', 'a', u'trump', u'presid', u'mean', u'for', u'current', u'intern', u'master', 's', u'student', 'on', 'an', 'f', u'visa']\n",
      "*****\n",
      "[u'what', 'is', u'the', u'step', 'by', u'step', u'guid', 'to', u'invest', 'in', u'share', u'market', u'what', u'would', u'happen', 'if', u'the', u'indian', u'govern', u'stole', u'the', u'kohinoor', u'koh', 'i', u'noor', u'diamond', u'back', u'how', u'can', u'internet', u'speed', 'be', u'increas', 'by', u'hack', u'through', u'dns', u'find', u'the', u'remaind', u'when', u'math', u'math', 'is', u'divid', 'by', u'which', u'fish', u'would', u'surviv', 'in', u'salt', u'water', 'i', 'm', 'a', u'tripl', u'capricorn', u'sun', u'moon', u'and', u'ascend', 'in', u'capricorn', u'what', u'doe', u'this', u'say', u'about', 'me', u'what', u'keep', u'childern', u'activ', u'and', u'far', u'from', u'phone', u'and', u'video', u'game', u'what', u'should', 'i', 'do', 'to', 'be', 'a', u'great', u'geologist', u'when', 'do', u'you', u'use', u'instead', 'of', u'and', u'how', 'do', 'i', u'hack', u'motorola', u'dcx', u'for', u'free', u'internet', u'what', u'are', u'some', 'of', u'the', u'thing', u'technician', u'can', u'tell', u'about', u'the', u'durabl', u'and', u'reliabl', 'of', u'laptop', u'and', u'it', u'compon', u'how', u'can', 'i', u'see', u'all', 'my', u'youtub', u'comment', u'how', u'can', u'you', u'make', u'physic', u'easi', 'to', u'learn', u'what', u'was', u'your', u'first', u'sexual', u'experi', u'what', u'are', u'the', u'law', 'to', u'chang', u'your', u'status', u'from', 'a', u'student', u'visa', 'to', 'a', u'green', u'card', 'in', u'the', 'us', u'how', 'do', u'they', u'compar', 'to', u'the', u'immigr', u'law', 'in', u'japan', u'how', u'will', 'a', u'trump', u'presid', u'affect', u'the', u'student', u'present', 'in', 'us', 'or', u'plan', 'to', u'studi', 'in', 'us']\n",
      "what , what\n",
      "----\n",
      "what\n",
      "----\n",
      "what\n",
      "----\n",
      "what\n",
      "----\n",
      "what\n",
      "w\n",
      "[u'w', u'w']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-903fb8bdcb89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0manalyze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mnewCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewCorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mxArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mcosinesimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    782\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "stemmer = LancasterStemmer()\n",
    "def words_in_string(word_list, a_string):\n",
    "    return set(word_list).intersection(a_string.split())\n",
    "def cosine(a0,a1):\n",
    "    dotp = np.dot(a0, a1)\n",
    "    mag0 = np.linalg.norm(a0)\n",
    "    mag1 = np.linalg.norm(a1)\n",
    "    cosineSim = dotp / (mag0* mag1)\n",
    "    return cosineSim\n",
    "cosinesimilarityarray=[]\n",
    "# print document1\n",
    "# print '*****'\n",
    "# print document2\n",
    "for a,b in zip(document1,document2):\n",
    "#     print a + ' , ' + b\n",
    "    corpus = []\n",
    "    newCorpus = []\n",
    "    for i,j in zip(a,b):\n",
    "#         print '----'\n",
    "#         print a\n",
    "        i = SnowballStemmer(\"english\").stem(i)\n",
    "        j = SnowballStemmer(\"english\").stem(j)\n",
    "#     print a[0]\n",
    "    corpus.append(a[0])\n",
    "    corpus.append(b[0])\n",
    "    corpus[0] = ' '.join(corpus[0]).lower()\n",
    "    corpus[1] = ' '.join(corpus[1]).lower()\n",
    "    punc_trans_table = {ord(c): None for c in string.punctuation}\n",
    "    corpus[0].translate(punc_trans_table)\n",
    "    corpus[1].translate(punc_trans_table)\n",
    "    corpus[0]= re.sub('[!\\[\\]@#\\\\\\.:?)(}{,$/]', '', corpus[0])\n",
    "    corpus[1]= re.sub('[!\\[\\]@#\\\\\\.:?)(}{,$/]', '', corpus[1])\n",
    "#     remove_digits = string.maketrans('', '', digits)\n",
    "    digit_trans_table = {ord(c): None for c in string.digits}\n",
    "    corpus[0] = corpus[0].translate(digit_trans_table)\n",
    "    corpus[1] = corpus[1].translate(digit_trans_table)\n",
    "#     string0 = words_in_string(words,corpus[0])\n",
    "#     string1 = words_in_string(words,corpus[1])\n",
    "    newCorpus.append(corpus[0])\n",
    "    newCorpus.append(corpus[1])\n",
    "    vectorizer = CountVectorizer(min_df=1) \n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    print newCorpus\n",
    "    X = vectorizer.fit_transform(newCorpus)\n",
    "    xArray = X.toarray()\n",
    "    cosinesimilarity = cosine(xArray[0],xArray[1])\n",
    "    cosinesimilarityarray.append(cosinesimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cosinesimilarityarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "for w in words:\n",
    "    if(d.check(w)==False):\n",
    "        if(d.suggest(w) is not None):\n",
    "            try:\n",
    "                w = d.suggest(w)[0]\n",
    "            except IndexError:\n",
    "                w=w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
