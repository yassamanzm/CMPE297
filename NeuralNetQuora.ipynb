{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"~/desktop/machinelearning/trainmini.csv\",encoding='latin-1').fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the story of Kohinoor (KohminusiminusN...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when \\[math\\]23^\\{24\\}\\[/ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Which one dissolve in water quikly sugar comma...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          question1  \\\n",
       "0   0  What is the step by step guide to invest in sh...   \n",
       "1   1  What is the story of Kohinoor (KohminusiminusN...   \n",
       "2   2  How can I increase the speed of my internet co...   \n",
       "3   3  Why am I mentally very lonely? How can I solve...   \n",
       "4   4  Which one dissolve in water quikly sugar comma...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when \\[math\\]23^\\{24\\}\\[/ma...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>What is the painting on this image?</td>\n",
       "      <td>What is this painting?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>Which are the major highways in California and...</td>\n",
       "      <td>Which are the major highways in California and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>What\\ s beyond our Universe?</td>\n",
       "      <td>If space is expanding comma  where does the ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>Is growing of hair a physical or a chemical ch...</td>\n",
       "      <td>Can a bald person ever grow their hair back?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>What is the difference between culture and his...</td>\n",
       "      <td>How is chili different between cultures?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          question1  \\\n",
       "495  495                What is the painting on this image?   \n",
       "496  496  Which are the major highways in California and...   \n",
       "497  497                       What\\ s beyond our Universe?   \n",
       "498  498  Is growing of hair a physical or a chemical ch...   \n",
       "499  499  What is the difference between culture and his...   \n",
       "\n",
       "                                             question2  is_duplicate  \n",
       "495                             What is this painting?             0  \n",
       "496  Which are the major highways in California and...             0  \n",
       "497  If space is expanding comma  where does the ne...             0  \n",
       "498       Can a bald person ever grow their hair back?             0  \n",
       "499           How is chili different between cultures?             0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 documents\n",
      "2 classes [0, 1]\n",
      "2040 unique stemmed words ['philosoph', 'donald', 'is', 'heavy', 'incid', 'degr', 'def', 'ac', 'yakshin', 'infosy', 'reason', 'snok', 'month', 'accept\\\\', 'caus', 'rec', 'nearminusdea', 'gre', 'dat', 'top', 'request\\\\', 'part', 'honest', 'ejac', 'sec', 'maintain', 'exclud', 'til', 'haz', 'firm', 'lak', '&', 'hous', 'porn', 'chief', 'pc', 'ctc', 'rop', 'cit', 'why', 'cancel', 'wat', 'digest', 'afraid', 'jrf', 'cant', 'cpagrip.com', 'sud', 'eg', 'remaind', 'g', 'instagram', 'tqwl', 'innov', 'cord', 'describ', 'nitk', 'halt', 'black', 'ther', 'geograph', 'lol', 'field', 'despit', 'pokì©mon', 'maul', 'chem', 'tel', 'fourminuswheelminusdr', 'bestfriend', 'harry', 'enough', 'democr', 'whe', 'headach', 'hand', 'pink', 'charg', 'old', '!', 'unharm', '109', 'dye', '19', '11th', 'than', 'emot', 'housework', 'dry', 'benef', ':', 'dead', 'film', 'recovery', 'back', 'gas', 'gun', 'propos', 'includ', 'confus', 'oberon', 'abstract', 'monst', 'impact', 'refund', 'which', 'freedom', 'overminussampl', 'gat', 'text', 'delet', 'en', 'turn', 'greatest', 'had', 'and', 'lord', 'key', 'imply', 'dimend', 'tycoon', 'retir', 'dream', '1\\\\', 'styl', 'tour', 'rely', 'feminin', 'krishn', 'chelmsfordminusessex', 'without', 'antisoc', 'air', 'thes', 'result', 'so', 'condit', 'smok', 'fem', 'rank', 'going', 'crust', 'assassin\\\\', 'ord', 'offens', 'shirt', 'mot', 'presid', 'tor', '8', 'comedy', 'cons', 'brok', 'dea', 'pet', 'haridw', 'bon', 'rit', 'cool', 'again', 'priv', 'pleas', 'yo', '[', 'disconnect', 'copyright', 'upload', 'stol', 'amazon', 'belt', 'speech', 'industry', 'spous', 'set', 'ex', 'queen', 'exminusboyfriend', 'anoth', 'along', 'aren\\\\', 'say', 'answ', 'gees', 'lightn', 'releas', 'angulars', 'attend', 'demonit', 'let', 'refus', 'platform', 'prosp', 'lady', 'pun', 'imrov', 'gandh', 'variableminussweep', 'jio', 'meet', 'flu', 'plagu', 'he', 'it\\\\', 'run', 'israil', 'oitnb', 'lon', 'grow', 'marvel', 'trust', 'mov', 'ub', 'light', 'to', 'sev', 'angul', 'choos', 'narcissist', '25', 'least', '14minus18', 'hap', 'greec', 'mem', 'motoroll', 'about', 'mak', 'country', 'coff', 'yogurt', 'suck', '50', 'facebook', 'mang', 'withdraw', 'finit', '6000', '1minus6', 'rav', 'piano', 'youtub', 'womb', 'kne', 'pocso', 'rub', 'sign', 'morn', 'her', 'french', 'diff', 'subsequ', 'how', 'we', 'mad', 'pak', 'candy', 'disord', 'ent', 'town', 'bastard', 'med', 'song', 'f1', 'voic', 'col', 'tiago', '23', 'migrain', 'han', 'torbox', 'fast', 'volt', 'perceiv', 'chant', 'å£3764', 'gym', 'screenshot', 'young', 'claim', 'grew', 'peac', 'reminusevalu', 'skil', 'hat', 'reward', 'on', 'join', 'beget', 'toothpast', 'may', 'applicant\\\\', 'simil', 'ªt', 'dish', 'ref', 'alleg', 'selfminushelp', 'depress', 'rostov', 'squ', 'com', 'cavem', 'mal', 'nonminusveg', 'gonulcel', '5', 'mor', 'yourself', 'fre', 'believ', 'ground', 'period', 'graph', 'through', 'assess', 'spain', 'who\\\\', 'howev', 'diesel', 'behavy', 'hp', 'buff', 'villain', 'doing', 'trad', 'reag', 'xbox', 'exchang', 'bit', 'breakup', 'opin', 'entrepr', 'planet', 'rob', 'socy', '1952', 'against', 'between', 'favourit', 'thank', 'chil', 'quotes/lesson', 'fact', 'shot', 'send', 'budget', 'hist', 'unclean\\\\', 'dissolv', 'candid', 'germ', 'rohingy', 'who', 'second', 'payp', 'dna', 'raspberry', 'miscarry', 'colleagu', 'dar', '1minus2', 'carn', 'robot', '200', 'pag', 'now', 'gravit', '151', 'ultr', 'forc', 'cre', 'particul', 'her/him', 'ee', 'trigonometry', 'enjoy', 'astrolog', 'suit', 'bangladesh', 'city', 'univers', 'gold', 'pot', 'discuss', 'kingdom', 'newby', 'method', 'cal', 'parallel', 'dns', 'fought', 'bald', 'best', 'inject', 'ind', 'jul', 'wheth', 'fear', 'noth', 'interview', 'abus', 'cockerel', 'vaccin', 'was', 'world', 'oxid', 'stat', 's', 'pradesh', 'ar', 'intellig', 'piram', 'onminuslin', 'instruct', 'ly', 'any', 'blu', 'board', 'unlock', 'dark', 'rsu', 'domain', 're', 'kid', 'infinit', 'neal', 'end', 'pm', 'math\\\\', 'traff', 'suff', 'bay', 'deduc', 'slav', 'study', 'pain', 'model\\\\', 'possess', 'overcom', 'look', 'websit', 'bjp', 'wor', 'backward', 'episod', 'sel', 'linkedin', 'kmc', 'od', 'lib', 'excus', 'delud', 'mahabharat', 'plu', 'platform/application', 'spee', 'math', 'big', 'reliev', 'europ', 'hum', 'car', 'o\\\\', 'wildl', 'ugc', 'contrast', 'ov', 'him', 'iit', 'death\\\\', 'motorol', 'cry', 'show', 'bminusschool', 'architect', 'etiquet', 'gf', 'play', 'ticket', 'minus3x\\\\', 'immun', 'fir', 'firefight', 'book', 'bullmastiff/husky', 'girl', 'quest', 'pup', 'rsi', 'too', 'think', '(', 'icon', '4x^2', 'ics', 'earphon', 'best/most', 'karnatak', 'util', 'being', 'seny', 'wrot', 'brand', 'laptop', 'monet', 'cor', 'get', 'could', 'cogn', 'org', 'gmail', 'swam', 'beyond', 'ò2', 'sarrainodu', 'upsc', 'kolkat', 'gps', 'teen', 'mit', 'stir', '13', 'hack', 'pi', 'lit', 'web', 'ecc', 'mantr', 'coug', 'prozac', '20s', 'childr', 'titl', 'ev', 'cpuminusz', '500rs', 'pict', 'alex', 'essay', 'journey', 'protagon', 'wee', 'dean', 'of', '4shared', 'bsu', '60000', 'cities\\\\', 'reloc', 'hour', 'cod', 'non', 'nichola', 'whatsap', 'masood', 'valu', 'try', 'it', 'fight', 'pro', 'clock', 'excess', 'or', 'flo', 'bef', 'ª', 'mil', 'sandeep', 'collect', 'ceremony', 'f', 'onsh', '1st', 'hindu', 'vel', 'opportun', 'syndrom', 'win', 'tempt', 'regardless', 'tip', 'cricket', 'colleg', 'what\\\\', 'kurukshetr', 'badg', 'aap', 'mystery', 'orang', 'guid', 'databas', 'bass', 'lat', 'educ', 'emoticon', 'visit', 'dunt', 'prim', 'psycholog', 'swiss', 'decathlet', '4g', 'triangl', 'docu', 'sooth', 'mood', 'confirm', '4000', 'hid', 'ic', 'brea', 'real', 'flash', 'assassin', 'good', 'hyderabad', 'convert', 'loc', 'inbuilt', 'disagr', 'che', 'hol', 'forgot', 'exh', 'mark', 'green', 'strain', 'friend', 'saf', 'happy', 'kevl', 'deport', 'jap', 'draw', 'potty', '3.8', 'school', 'war', 'ide', 'vis', 'bullet', 'upon', 'cupcak', 'maj', 'stop', 'tool', 'lipid', 'espec', 'daniel', 'abid', 'ms', 'when', 'vad', 'engin', 'expand', 'moto', 'polynom', 'barod', 'britain', 'quickest', 'prison', 'tal', '{', 'sal', 'eat', 'tax', 'across', 'pric', 'jackpot', 'bal', 'mag', 'autom', 'want', 'movy', 'bir', 'review', 'regul', 'benefit', 'unit', 'divorc', 'valparaiso', 'vs', 'rel', 'cs', 'softw', 'elect', 'girlfriend', 'kore', 'others\\\\', 'nd', 'deism', 'quit', 'miss', 'criminal\\\\', 'blood', 'chart', 'kapil', 'his', 'gain', 'boat', 'determin', 'neut', 'six', 'soc', '6d', 'clash', 'terr', 'mezzo', 'attract', 'i10', 'peopl', 'god', '2nd', 'airy', 'nos', 'inkjet', 'kil', 'scal', 'with', 'spit', 'mcdonald\\\\', 'sea', 'calend', 'read', 'person', 'scen', 'cur', 'easy', 'dminuslux', 'put', 'langu', 'seem', 'tea', 'two', 'wed', '2016', 'boy', 'ap', 'sold', 'percentpl', 'pap', 'purpos', 'ready', 'build', 'out', 'boost', '1', 'apply', 'account', 'riyadh', 'stor', 'govern', 'both', 'habit', 'vick', 'vpn', 'radio', 'alon', 'puk', '15000', 'cocain', 'step', 'doesn\\\\', 'urg', '15k', 'ir', 'ai', 'shortl', 'chat', 'omn', 'day', 'dislik', 'first', 'cont', 'nee', 'bachel', 'inr', 'sugar\\\\', 'much', 'talk', 'rowl', 'custom', 'ªs', 'ascend', 'worry', 'google.com', 'stal', 'as', 'continu', 'extend', 'hunt', 'cse', 'kidnap', 'not', 'bitcoin', 'london', 'respect', 'altern', 'lying', 'presp', 'hostel', 'tabl', 'dant', 'asleep', 'pls', 'fig', 'gross', 'energy', 'besid', 'leic', 'transf', 'switzerland', 'gettysburg', 'ear', 'surathk', 'rot', 'dream\\\\', 'liquid', 'sens', 'abud', 'tiny', 'rup', '99modulo', 'highway', 'search', 'lik', 'de', 'person\\\\', 'gap', 'warry', 'pol', 'il', 'can\\\\', 'comm', '6', 'work', 'canon', 'waveclu', 'marrow', 'self', 'assocy', 'healthy', 'and\\\\', 'diet', 'lack', 'internet', 'mix', 'twominusmonthminusold', 'origin', 'blackl', 'malar', 'tap', 'rebel', 'a', 'am', 'friend\\\\', 'height', 'sourc', 'compl', 'but', 'jeremy', 'scary', 'hir', 'room', 'earthquak', 'opt', '\\x89â_5000', 'affleck', 'tak', 'diagnos', 'lov', 'tripl', 'medicin', 'fing', 'lee', 'analys', 'suggest', 'low', 'ashutosh', 'spirit', 'divid', 'kiss', 'lyr', 'shoun', 'buy', 'dms', 'c', 'rom', 'calender/scheduling/booking', 'escap', 'spot', 'go', 'complet', 'stewart', 'project', 'ink', 'feat', 'doe', 'ulc', 'in', 'bas', 'nicm', 'paycheck', 'mind', 'smart', 'wet', 'clinton', 'ma', 'evil', 'difficult', 'cas', 'm.k.gandhi', 'adjust', 'is\\\\', 'break', '}', 'secret', 'rexnord', 'inform', 'methamphetamin', 'germany', 'bed', 'fanboy', 'im', 'len', 'holy', 'con', 'wit', 'skrill', 'walt', 'fm', 'earn', 'unwrit', 'bipr', 'surv', 'swim', 'emp', 'mantra', 'hatter\\\\', 'apart', 'with\\\\', 'sport', 'whom', 'nuclear', 'deep', 'whil', 'someth', '24\\\\', 'avoid', 'chos', 'bucket', 'away', 'weight', 'stan', 'sigmaminusaldrich', 'funniest', 'reduc', 'word', 'ray', 'desp', 'keral', 'i', 'delicy', 'new', 'trustworthy', 'quickbook', 'hold', 'abc', 'competit', 'switch', 'year\\\\', 'prov', 'hav', 'madra', '000', 'blasphem', 'ehrhardt', 'an', 'he/sh', 'econom', 'x', 'child\\\\', 'also', 'est', 'syllab', 'wireless', 'els', 'every', 'norm', 'immers', 'prep', 'lost', 'watch', 'start', 'quit/stop', '14', 'childern', 'vs.', 'howard', 'nobody', 'thi', 'poss', 'kind', 'clon', 'learn', 'setup', 'tang', 'reply', 'masturb', 'news\\\\', 'freem', 'u.s.', 'pow', 'me', 'zealand', 'membr', 'sweet', 'trick', 'hang', 'andhr', 'boyfriend', 'wok', 'off', 'loos', 'can', 'year', 'dying', 'bisex', 'batch', 'dam', 'injury', 'request', 'begin', 'io', 'spec', 'â', 'wish', '2\\\\', 'jeal', 'snow', 'immigr', 'niro', 'popul', 'togeth', 'exampl', 'incom', 'aerodynam', 'bestmytest.com', '36x\\\\', 'point', 'j', 'como', 'funct', 'adv', 'do', 'ring', 'dprk', 'yod', 'pacino', 'kamchatc', 'bcom', 'cologn', 'password', 'mountain', 'som', 'ask', 'thirteen', 'dur', 'onlin', 'franch', 'sint', 'workshop', 'dorsey', 'strong', 'mech', 'die', 'into', ';', '2012', 'execut', 'e.g', 'the', 'fminus14', 'expect', 'hind', 'arvind', 'invitations\\\\', 'from', 'serv', 'midsum', 'program', 'plan', 'mercy', 'pay', 'follic', 'commerc', 'ted', '/', 'homeland', 'effect', 'payminusaft', 'iec', 'surg', 'quik', 'nse', 'such', 'x^\\\\', 'ocr', 'kì¦ln', 'ranch', 'kejriw', 'android', 'kv', 'money', 'nevertheless', 'uk', 'gam', 'chick', 'instal', '3t', 'htc', 'deuteromycot', 'everywh', 'aiim', 'pend', 'leg', '$', 'tripod', 'around', 'burmaminusrohingy', 'amount', '1000', 'ath', 'requir', 'minim', 'rul', 'catelyn', 'ronald', 'mod', 'sect', 'vivekanand', 'coast', 'trigonomet', 'week', '20', 'drug', 'horsepow', 'clan', 'term', 'night', 'ign', '/math\\\\', 'breakfast', 'frict', '11', 'jehovah', 'hair', 'rest', 'explain', 'season', 'powel', 'email', 'tie', 'breath', 'replac', 'instead', 'shop', 'spam', 'stuffy', 'strongest', 'be', 'smal', 'gradu', 'mac', 'aft', 'rid', '10', 'gdp', 'produc', 'job', 'introduc', 'doct', 'don\\\\', 'gambl', 'juny', 'fund', 'conf', '2017', 'long', 'stag', '1.5', 'revers', 'intern', 'one\\\\', 'krist', 'fish', 'champ', 'ao', 'nam', 'pop', 'christians', 'feel', 'jem', 'hil', 'cambod', 'machin', 'what', 'stream', 'tcs', 'mus', 'company', 'posit', 'scratch', 'guy', 'kg', 'lead', 'consult', 'fair', 'fix', 'sci', 'm', 'shap', 'op', 'dvd', 'would', 'many', 'debeak', 'vap', 'spons', '9/11', 'y', 'nor', 'middl', 'journ', 'contribut', 'stud', 'predict', 'calcul', 'glob', 'just', 'breast', 'mex', 'nev', 'zero', '17500', 'less', 'nba', 'clim', 'un', 'remind', 'ca', 'mon', 'span', 'caltech', 'grand', 'abl', 'fet', 'catch', 'richest', 'mminusth', 'robert', 'download', 'acc', 'labrad', 'amus', 'should', 'investig', 'vitamin', 'charact', 'expens', 'builtminusin', 'earth\\\\', 'dog', 'und', 'compress', 'ben', 'straits', 'control', 'others.\\\\', 'sex', 'salt', 'conserv', 'eq', 'engl', 'went', 'algorithm', 'rond', 'market', '2', 'titan', 'azh', 'weld', 'reg', 'wrong', 'shar', 'said', 'stomach', 'cr', 'rout', 'comet', 'gem', 'bermud', 'fal', 'rocket', 'thrones', '30', 'a.m.', 'ant', 'regain', 'burm', 'mobl', 'child', 'capricorn', 'regard', '9', 'lottery', 'sod', 'react', 'cognit', 's\\\\frac\\\\', 'corgiminushusky', 'resist', 'phon', 'interest', 'story', 'nic', 'fict', 'shahrukh', 'fed', ']', 'for', 'cult', 'los', 'stock', 'lesson', 'jingl', 'process', 'then', 'class', 'jailbreak', 'nonminusbit', 'test', 'illeg', 'dc', 'their', 'follow', '\\x89â_500', 'overlap', 'pizz', 'polyest', '1822', 'surgeon', 'morg', 'channel', 'prop', 'riv', 'wing', 'sukany', 'mortg', 'fresh', 'know', 'adult', 'invest', 'act', 'elig', 'drown', 'shepherd', 'heat', 'been', 'got', 'mess', 'der', '4', 'institut', 'billionair', 'offsh', 'becom', 'geolog', '70', 'sep', 'aspir', 'strategies', 'shel', 'otherw', 'renown', 'fulfil', 'main', 'wer', 'b', 'my', 'grab', 'toothbrush', 'pres', 'attempt', 'success', 'combin', 'bloomberg', 'found', 'harm', 'commercial/clip', 'christian', 'argu', 'civil', 'linux', 'increas', 'cc', 'construct', 'japanes', 'mr.', 'shouldn\\\\', 'they', 'myth', 'ek', 'excit', 'gift', 'sim', 'adopt', 'down', 'seen', 'singl', 'unhealthy', 'beneath', 'chines', 'kar', 'inst', 'gulf', 'startup', 'yantr', 'obb/data', 'attack', 'accord', 'fin', '5s', 'leav', 'pound', 'wil', 'post', 'crack', 'canad', 'oth', 'regret', 'cri', 'access', 'pregn', 'mtech', 'see', 'googl', 'di', 'mindminusblow', 'pack', 'actress', 'admit', 'ban', 'glass', 'very', 'tf64simc4', 'iim', 'support', 'meth', 'most', 'grad', 'contact', 'termin', 'mail', 'short', 'nak', 'pitbul', 'fil', 'forget', '.an', 'us', 'asah', 'puppet', 'cameo', 'drink', 'scrapping', 'wors', 'pencil', 'canterbury', 'cbse', 'biolog', 'fd', 'pass', 'profit', 'western', 'someon', 'tru', 'comparison', 'cut', 'aircraft', 'potass', 'devast', 'jen', 'fram', '24', 'petrol', 'ï', 'anym', 'grav', 'qual', 'affect', 'sent', 'dumb', 'horcrux', 'multiminusclass', 'teach', 'recruit', 'dai', 'rap', 'limit', 'techn', 'shak', 'neutron', 'mytholog', 'recov', 'publ', 'asteroid', 'invit', 'she', 'nadu', 'prank', 'perform', 'evernot', 'last', 'list', 'cap', 'cotton', 'upvot', 'anyon', 'hollywood', 'system', 'fiv', 'subtitl', 'trend', 'you', 'patch', 'batm', 'chang', 'portugues', 'jon', 'numb', 'extract', 'tamil', 'love\\\\', 'khan', 'minut', 'restless', 'ment', 'view', 'agnost', 'edit', 'exocytos', 'greek', 'you\\\\', 'raby', '1000rs', 'polo', 'lawr', 'narendr', 'brooklyn', 'decid', 'map', 'correct', 'accel', 'transport', 'sem', 'sad', 'sug', 'sigm', 'improv', 'cert', 'thriller', 'tim', 'ur', 'skyp', 'conduc', 'cmyk', 'corrupt', 'assault', '19minusyearminusold', 'bet', 'ad', 'contract', 'm.tech', 'decl', 'obam', 'our', 'compon', 'reach', 'headphon', 'lucid', 'bin', 'gpa', 'cram', 'spend', 'bil', 'eyesight', '12', 'comp', 'cost', '21', 'disrupt', 'form', 'pity', 'bank', 'scor', 'block', 'diamond', 'that', 'whol', 'strip', 'ten', 'heal', 'doll', 'agree', 'insid', 'cmc', 'party', 'anxy', 'law', 'heard', 'affy', '3d', 'antenn', 'enh', 'winston', 'if', '4.1.1', 'forward', 'biggest', 'wom', 'allow', 'endocytos', 'trump', 'dcx3400', 'sun\\\\', 'disadv', 'hom', 'video', 'californ', 'pump', 'thick', 'gen', 'birthday', 'partn', 'crud', 'bangl', 'lect', 'standard', 'eminusmail', 'novel', 'homework', 'giv', 'wel', 'tast', 'jerry', 'tongu', 'sery', 'already', 'nomin', 'everyth', 'head', 'writ', 'list\\\\', '@', 'brief', '1984\\\\', 'hurt', 'reserv', 'atm', 'driv', '\\\\', 'quickbooks\\\\', 'stil', 'gst', 'distribut', 'typescrib', 'next', 'garb', 'quor', 'famy', 'no', 'hit', 'approxim', 'screen', 'print', 'incest', 'solv', 'address', 'himalay', 'must', 'chin', 'issu', 'deal', 'fab', 'touch', 'propel', 'j.k', 'high', 'mean', 'mass', 'dmart', 'way', 'swept', 'cree', 'addict', 'cruz', 'launch', 'digit', 'daddy', 'biotechnolog', 'destruct', '.', 'tent', 'sanct', 'star', 'phrase', 'to\\\\', 'ey', 'twit', 'macbook', 'oddminusev', 'liv', 'far', 'polit', 'fee', 'min', 'island', 'quot', 'phys', 'select', 'few', 'respons', 'structure', 'ag', 'campaign', 'auto', 'id', 'etc', 'by', 'body', 'shin', '3\\\\', 'manip', 'confid', 'nightm', 'branch', 'employ', 'leagu', 'escort', 'salad', '60k', 'profil', 'beard', 'battl', 'man', 'cook', 'night\\\\', 'crush', 'm.phil', 'srm', 'avail', 'level', 'rousey', 'knowledg', 'satir', 'samrud', '0', 'ann', 'gend', 'muslim', 'rd', 'train', 'al', 'repair', 'below', 'taffy', 'network', 'coupl', 'fun', 'cheapest', 'memb', 'lif', 'whit', 'kohino', 'sound', 'apm', '#', 'virgin', 'folk', 'pursu', 'columb', 'net', 'recip', 'hobby', 'men', 'ip', 'imb', 'help', 'proc', 'initiative/standards\\\\', 'moon', '7', 'attorney', '.my', 'thos', 'expl', 'obtain', 'consid', 'rah', 'molec', 'atlant', 'easiest', 'bad', 'i\\\\', 'duk', 'nat', 'prev', 'sav', 'food', 'expery', 'av', '18', 'congress', 'saltw', 'pref', 'dev', 'clar', '\\x89â_1000', 'palpatin', 'knight', 'int', 'ignit', 'patanjal', 'exact', 'wak', 'ckwl', 'shaquil', 'and/or', 'iphon', 'seinfeld', 'harvard', 'tempera', 'pilot', 'latest', 'comput', 'plac', 'freel', 'vtu', 'keep', 'slit', 'dec', 'check', 'jack', 'cov', 'understand', 'shut', 'rohingya\\\\', 'rac', 'anim', 'estim', 'road', 'shav', 'among', 'he\\\\', 'mast', 've', '23^\\\\', 'jok', 'typ', 'myself', 'rais', 'prom', 'framework', 'husband', 'past', 'stereotyp', 'mat', 'wif', 'entry', 'elit', '...', 'convint', '500', 'nonminuscod', 'lift', 'ris', 'jav', 'design', 'kickass', 'red', 'stick', 'stant', 'alway', 'has', 'purchas', 'git', 'railway', 'volksw', 'walk', '÷need', 'wait', 'each', 'becaus', 'yojan', 'tatk', 'norway', 'noon', 'reject', 'beer', 'appoint', 'inconsist', 'z630s', 'maheshwar', 'exam', 'fak', 'protein', 'travel', 'textbook', 'wher', 'egypt', 'disqual', 'surfac', 'technolog', 'bluetoo', 'humanit', '\\x9d', 'smsminusreminders', 'rich', '3', 'busy', 'texa', 'raisin', 'jaishminuseminusmohammad', 'import', 'ok', 'right', 'ful', '375', 'freak', 'sun', 'rs', 'avocado', 'airtel', 'bor', ')', 'ktm', 'develop', 'genuin', 'republ', 'acquir', 'surgery', 'rememb', 'expos', 'exminusgirlfriend', 'ipc', 'fut', 'rep', 'jump', 'trichy', 'log', '2019', 'discharg', 'oil', 'occ', 'card', 'speak', 'them', '5kms', 'milk', 'cours', 'sit', 'hysteria\\\\', 'appl', 'up', 'ned\\\\', 'express', 'sufficy', 'cgpa', 'don', 'interstell', '6minus11years', '11minus14', 'hd', 'cel', 'chocol', 'object\\\\', 'common', 'coworkers/bosse', 'carbon', 'fresnel', 'almost', 'unrefrig', 'connect', 'haven\\\\', 'pg', 'highest', 'xminus9\\\\', 'label', 'resolv', 'might', 'flea', 'spac', 'whey', 'turk', 'boy/girl', 'at', 'chequ', 'legend', 'did', '1minus855minus425minus3768', 'fath', 'tv', 'thing', 'demonet', 'paint', 'influ', 'weigh', 'sam', 'israel', '10/6', 'isro', 'cloth', 'kohminusiminusno', 'find', 'curs', 't', 's\\\\dfrac\\\\', 'delhi\\\\', 'collecting\\\\', 'dress', 'templ', '40', 'rbi', 'tre', 'theref', 'weapon', 'own', 'deposit', 'impl']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our training data\n",
    "for index, row in df.iterrows():\n",
    "    # tokenize each word in the sentence\n",
    "    w = nltk.word_tokenize(row['question1'])\n",
    "    w2 = nltk.word_tokenize(row['question2'])\n",
    "    # add to our words list\n",
    "    words.extend(w)\n",
    "    words.extend(w2)\n",
    "    # add to documents in our corpus\n",
    "    documents.append((w, row['is_duplicate']))\n",
    "    documents.append((w2, row['is_duplicate']))\n",
    "    # add to our classes list\n",
    "    if row['is_duplicate'] not in classes:\n",
    "        classes.append(row['is_duplicate'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "\n",
    "words = list(set(words))\n",
    "\n",
    "# remove duplicates\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'step', 'by', 'step', 'guid', 'to', 'invest', 'in', 'shar', 'market', 'in', 'ind', '?']\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    training.append(bag)\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)\n",
    "\n",
    "# sample training/output\n",
    "i = 0\n",
    "w = documents[i][0]\n",
    "print ([stemmer.stem(word.lower()) for word in w])\n",
    "print (training[i])\n",
    "print (output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    " \n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "def think(sentence, show_details=False):\n",
    "    x = bow(sentence.lower(), words, show_details)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our bag of words\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = sigmoid(np.dot(l0, synapse_0))\n",
    "    # output layer\n",
    "    l2 = sigmoid(np.dot(l1, synapse_1))\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n",
    "\n",
    "    print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n",
    "    print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, len(classes)) )\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                \n",
    "        if(dropout):\n",
    "            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = y - layer_2\n",
    "        if (j% 10000) == 0 and j > 5000:\n",
    "            # if this 10k iteration's error is greater than the last iteration, break out\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "                \n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # persist synapses\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    synapse_file = \"synapses.json\"\n",
    "\n",
    "    with open(synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", synapse_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(training)\n",
    "y = np.array(output)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train(X, y, hidden_neurons=20, alpha=0.1, epochs=100000, dropout=False, dropout_percent=0.2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.2\n",
    "# load our calculated synapse values\n",
    "synapse_file = 'synapses.json' \n",
    "with open(synapse_file) as data_file: \n",
    "    synapse = json.load(data_file) \n",
    "    synapse_0 = np.asarray(synapse['synapse0']) \n",
    "    synapse_1 = np.asarray(synapse['synapse1'])\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = think(sentence, show_details)\n",
    "\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    print (\"%s \\n classification: %s\" % (sentence, return_results))\n",
    "    return return_results\n",
    "\n",
    "classify(\"sudo make me a sandwich\",\"make me a burger\")\n",
    "print()\n",
    "classify(\"good day\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
